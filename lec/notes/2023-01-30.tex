\documentclass[12pt, leqno]{article}
\input{common}

\begin{document}
\hdr{2023-01-30}

\section{Matrices and mappings}

A matrix represents a mapping between two vector spaces.  That is, if
$L : \mathcal{V} \rightarrow \mathcal{W}$ is a linear map, then the
associated matrix $A$ with respect to bases $V$ and $W$ satisfies
$A = W^{-1} L V$.  The same linear mapping corresponds to different
matrices depending on the choices of basis.  But matrices can represent
several other types of mappings as well.  Over the course of this
class, we will see several interpretations of matrices:
\begin{itemize}
\item {\bf Linear maps}.  A map $L : \calV \rightarrow \calW$ is
  linear if $L(x+y) = Lx + Ly$ and $L(\alpha x) = \alpha Lx$.
  The corresponding matrix is $A = W^{-1} L V$.
\item {\bf Linear operators}.  A linear map from a space to itself
  ($L : \calV \rightarrow \calV$) is a linear operator.
  The corresponding (square) matrix is $A = V^{-1} L V$.
\item {\bf Bilinear forms}.  A map
  $a : \calV \times \calW \rightarrow \bbR$ (or $\bbC$ for complex
  spaces) is bilinear if it is linear in both slots:
  $a(\alpha u+v, w) = \alpha a(u,w) + a(v,w)$ and
  $a(v, \alpha u + w) = \alpha a(v,u) + a(v,w)$.
  The corresponding matrix has elements $A_{ij} = a(v_i, w_j)$;
  if $v = Vc$ and $w = Wd$ then $a(v,w) = d^T A c$.

  We call a bilinear form on $\calV \times \calV$ {\em symmetric} if
  $a(v,w) = a(w,v)$; in this case, the corresponding matrix $A$ is
  also symmetric ($A = A^T$).  A symmetric form and the corresponding
  matrix are called {\em positive semi-definite} if $a(v,v) \geq 0$
  for all $v$.  The form and matrix are {\em positive definite} if
  $a(v,v) > 0$ for any $v \neq 0$.

  A {\em skew-symmetric} matrix ($A = -A^T$) corresponds to a
  skew-symmetric or anti-symmetric bilinear form,
  i.e.~$a(v,w) = -a(w,v)$.
\item {\bf Sesquilinear forms}.  A map
  $a : \calV \times \calW \rightarrow \bbC$
  (where $\calV$ and $\calW$ are complex vector
  spaces) is sesquilinear if it is linear in the first slot and
  the conjugate is linear in the second slot:
  $a(\alpha u+v, w) = \alpha a(u,w) + a(v,w)$ and
  $a(v, \alpha u + w) = \bar{\alpha} a(v,u) + a(v,w)$.
  The matrix has elements $A_{ij} = a(v_i, w_j)$;
  if $v = Vc$ and $w = Wd$ then $a(v,w) = d^* A c$.

  We call a sesquilinear form on $\calV \times \calV$ {\em Hermitian} if
  $a(v,w) = a(w,v)$; in this case, the corresponding matrix $A$ is
  also Hermitian ($A = A^*$).  A Hermitian form and the corresponding
  matrix are called {\em positive semi-definite} if $a(v,v) \geq 0$
  for all $v$.  The form and matrix are {\em positive definite} if
  $a(v,v) > 0$ for any $v \neq 0$.

  A {\em skew-Hermitian} matrix
  ($A = -A^*$) corresponds to a skew-Hermitian or anti-Hermitian bilinear
  form, i.e.~$a(v,w) = -a(w,v)$.
\item {\bf Quadratic forms}.  A quadratic form $\phi : \calV
  \rightarrow \bbR$ (or $\bbC$) is a homogeneous quadratic function
  on $\calV$, i.e.~$\phi(\alpha v) = |\alpha|^2 \phi(v)$ for which the
  map $b(v,w) = \phi(v+w) - \phi(v) - \phi(w)$ is bilinear.
  Any quadratic form on a finite-dimensional space can be
  represented as $c^* A c$ where $c$ is the coefficient vector for
  some Hermitian matrix $A$.  The formula for the elements of $A$
  given $\phi$ is left as an exercise.
\end{itemize}
We care about linear maps and linear operators almost everywhere, and
most students come out of a first linear algebra class with some
notion that these are important.  But apart from very standard
examples (inner products and norms), many students have only a vague
notion of what a bilinear form, sesquilinear form, or quadratic form
might be.  Bilinear forms and sesquilinear forms show up when we
discuss large-scale solvers based on projection methods.  Quadratic
forms are important in optimization, physics (where they often
represent energy), and statistics (e.g.~for understanding variance and
covariance).

\subsection{Matrix norms}

The space of matrices forms a vector space; and, as with other vector
spaces, it makes sense to talk about norms.  In particular, we
frequently want norms that are {\em consistent} with vector norms
on the range and domain spaces; that is, for any $w$ and $v$,
we want
\[
  w = Av \implies \|w\| \leq \|A\| \|v\|.
\]
One ``obvious'' consistent norm is the {\em Frobenius norm},
\[
  \|A\|_F^2 = \sum_{i,j} a_{ij}^2.
\]
Even more useful are {\em induced norms} (or {\em operator norms})
\[
  \|A\| = \sup_{v \neq 0} \frac{\|Av\|}{\|v\|} = \sup_{\|v\|=1} \|Av\|.
\]
The induced norms corresponding to the vector 1-norm and $\infty$-norm
are
\begin{align*}
  \|A\|_1 &= \max_j \sum_i |a_{ij}| \quad \mbox{(max abs column sum)}\\
  \|A\|_\infty &= \max_i \sum_j |a_{ij}| \quad \mbox{(max abs row sum)}
\end{align*}
The norm induced by the vector Euclidean norm (variously called
the matrix 2-norm or the spectral norm) is more complicated.

The Frobenius norm and the matrix 2-norm are both {\em orthogonally
  invariant} (or {\em unitarily invariant} in a complex vector space.
That is, if $Q$ is a square matrix with $Q^* = Q^{-1}$ (an orthogonal
or unitary matrix) of the appropriate dimensions
\begin{align*}
  \|QA\|_F &= \|A\|_F, &
  \|AQ\|_F &= \|A\|_F, \\
  \|QA\|_2 &= \|A\|_2, &
  \|AQ\|_2 &= \|A\|_2.
\end{align*}
This property will turn out to be frequently useful throughout the course.

\subsection{Decompositions and canonical forms}

{\em Matrix decompositions} (also known as
{\em matrix factorizations}) are central to numerical linear algebra.
We will get to know six such factorizations well:
\begin{itemize}
\item
  $PA = LU$ (a.k.a.~Gaussian elimination).  Here $L$ is unit lower
  triangular (triangular with 1 along the main diagonal), $U$ is upper
  triangular, and $P$ is a permutation matrix.
\item
  $A = LL^*$ (a.k.a.~Cholesky factorization).  Here $A$ is Hermitian
  and positive definite, and $L$ is a lower triangular matrix.
\item
  $A = QR$ (a.k.a.~QR decomposition).  Here $Q$ has orthonormal
  columns and $R$ is upper triangular.  If we think of the columns
  of $A$ as a basis, QR decomposition corresponds to the Gram-Schmidt
  orthogonalization process you have likely seen in the past (though
  we rarely compute with Gram-Schmidt).
\item
  $A = U \Sigma V^*$ (a.k.a.~the singular value decomposition or SVD).
  Here $U$ and $V$ have orthonormal columns and $\Sigma$ is diagonal
  with non-negative entries.
\item
  $A = Q \Lambda Q^*$ (a.k.a.~symmetric eigendecomposition).  Here $A$
  is Hermitian (symmetric in the real case), $Q$ is orthogonal or
  unitary, and $\Lambda$ is a diagonal matrix with real numbers on the
  diagonal.
\item
  $A = QTQ^*$ (a.k.a.~Schur form).  Here $A$ is a square matrix, $Q$
  is orthogonal or unitary, and $T$ is upper triangular (or nearly
  so).
\end{itemize}

The last three of these decompositions correspond to
{\em canonical forms} for abstract operators.  That is, we can view
these decompositions as finding bases in which the matrix
representation of some operator or form is particularly simple.
More particularly:
\begin{itemize}
\item {\bf SVD}:
  For any linear mapping $L : \mathcal{V} \rightarrow \mathcal{W}$,
  there are orthonormal bases for the two spaces such that the
  corresponding matrix is diagonal
\item {\bf Symmetric eigendecomposition}:
  For any Hermitian sesquilinear map on an inner product space, there
  is an orthonormal basis for the space such that the matrix
  representation is diagonal.
\item {\bf Schur form}:
  For any linear operator $L : \mathcal{V} \rightarrow \mathcal{V}$,
  there is an orthonormal basis for the space such that the matrix
  representation is upper triangular.  Equivalently, if
  $\{u_1, \ldots, u_n\}$ is the basis in question,
  then $\operatorname{sp}(\{u_j\}_{j=1}^k)$ is an
  {\em invariant subspace} for each $1 \leq k \leq n$.
\end{itemize}
The Schur form turns out to be better for numerical work than the
Jordan canonical form that you should have seen in an earlier class.
We will discuss this in more detail when we discuss eigenvalue
problems.

\subsection{The SVD and the 2-norm}

The singular value decomposition is useful for a variety of reasons;
we close off the lecture by showing one such use.

Suppose $A = U \Sigma V^*$ is the singular value decomposition of some
matrix.  Using orthogonal invariance (unitary invariance) of the
2-norm, we have
\[
  \|A\|_2 = \|U^* A V\|_2 = \|\Sigma_2\|,
\]
i.e.~
\[
  \|A\|_2 = \max_{\|v\|^2 = 1} \frac{\sum_j \sigma_j |v_j|^2}{\sum |v_j|^2}.
\]
That is, the spectral norm is the largest weighted average of the
singular values, which is the same as just the largest singular value.

The small singular values also have a meaning.  If $A$ is a square,
invertible matrix then
\[
  \|A^{-1}\|_2 = \|V \Sigma^{-1} U^*\|_2 = \|\Sigma_{-1}\|_2,
\]
i.e.~$\|A^{-1}|_2$ is the inverse of the smallest singular value of $A$.

The smallest singular value of a nonsingular matrix $A$ can also be
interpreted as the ``distance to singularity'': if $\sigma_n$ is the
smallest singular value of $A$, then there is a matrix $E$ such that
$\|E\|_2 = \sigma_n$ and $A+E$ is singular; and there is no such
matrix with smaller norm.

These facts about the singular value decomposition are worth
pondering, as they will be particularly useful in the next lecture
when we ponder sensitivity and conditioning.

\end{document}
